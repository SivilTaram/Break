# Break: A Question Understanding Benchmark

<img align="left" src="images/hammer_and_anvil-1.png" height="100"></img>
Break is a human annotated dataset of natural language questions and their Question Decomposition Meaning Representations (QDMRs). Break consists of 83,978 examples sampled from 10 question answering datasets over text, images and databases.
This repository contains the Break dataset along with information on the exact data format.

For more details check out our TACL paper ["Break It Down: A Question Understanding Benchmark"](https://arxiv.org/), and [website](https://allenai.github.io/Break/).



* **Key Links**
	* **Break Dataset**: [https://github.com/allenai/Break/tree/master/break_dataset](https://github.com/allenai/Break/tree/master/break_dataset)
	* **Paper**: ["Break It Down: A Question Understanding Benchmark"
](https://arxiv.org/)
	* **Leaderboard**:  [Coming Soon](https://leaderboard.allenai.org/)
	* **Website**: [https://allenai.github.io/Break/](https://allenai.github.io/Break/)



* **Question Answering Datasets**
The Break dataset contains questions from the following 10 datasets: 
[Academic](https://), [ATIS](https://), [GeoQuery](https://), [Spider](https://), [CLEVR-humans](https://), [NLVR2](https://), [Academic](https://), [CommunityQA](https://), [ComplexWebQuestions](https://), [DROP](https://) and [HotpotQA](https://).

### Changelog

- `1/14/2020` The full dataset, models and entire codebase will be officially released following final paper acceptance.


